---
layout: post
title:  "Write deep learning models in a few lines of code with fastai"
date:   2021-07-12
tags: 
    - data science
    - deep learning
    - python
    - artificial intelligence 
author: Julian Garratt 
image: /assets/images/blog/2021-07-15/cover.jpg
excerpt_separator: <!--more-->
---
Have you ever considered getting started in deep learning but instead got frustrated by the steep learning curve and huge body of knowledge required to even start hacking up a simple "hello world" program in Pytorch? Thanks to like mind individuals, fastai was created in mind to provide high-level api access, making the creation of state of the art deep neural networks as easy as a couple of lines of code.
<!--more-->

Now, if you believe that I'm over embellishing, I'm not kidding. To give a hint of how high level fastai is, consider these two snippets of code, I'll let you guess which one is from fastai and which one is from Pytorch.

<p style="text-align: center;"> Exhibit A </p>
![Code](/assets/images/blog/2021-07-15/pytorch.png "Exhibit A")
<br><br>
<p style="text-align: center;"> Exhibit B </p>
![Code](/assets/images/blog/2021-07-15/fastai.png "Exhibit B")
<br>

As you may or may not have guessed, Exhibit B was code from the fastai package. Specifcally, both peices of code are used to fit the model and although this might just be one aspect of building a model, fastai comes prepared with easy data loading including tokenisation for NLP problems and automatic image transformation for vision data as well as highly customisable architectures for neural networks including GANs and RNNs.

# A high level overview of fastai
Although fastai comes pre-packed with numerous features, taking a look under the hood reveals a bottom-up framework ready to tackle the most common deep learning problems. It's also important to note that although fastai leverages packages like Pytorch and Scapy, the core developers (i.e. one of the founders Jeremy Howard) notes that anything that was not sufficient, the company re-designed and re-developed itself. For more info check out this [super instructional overview](https://www.youtube.com/watch?v=bHVqO5YyNbU) of fastai.

<div style="text-align: center"> <img src="/assets/images/blog/2021-07-15/structure.png" width="500" height="600"/> </div>

However, if you're like most people who glanced at the above diagram and yawned, getting a perfect understanding of a machine learning api may not be extremely important. Instead, take these next few paragraphs as a tl;dr, and if you're interested I'll add more resources for those who are keen to get into the weeds.

Speaking of high level overview, on the above diagram, sitting at the top is the holy-grail of the fastai package, their "ready-to-go" applications for data in vision, text and tabular (collab refers to collaborative filtering which is effictively a sub-application of tabular). However, applications are only nice if you don't know how to use them.

If you think of the main procedures of a model being, collecting data, training, and predicting (with preprocesssing, cross-validation etc. all sprinkled in between), fastai treats their applications as the ingredients with the final product being easily configurable and interchangeable model structures.

## Loading data
Accross text, vision and tabular data, fastai utilises the same highlevel data building process. At it's core, fastai's "data block" API takes the ease away from manually splitting data up into batches as well as preprocessing. Initally, the API might seem daunting but things become easier if you realise that the API was created for the sole purpose of flexibility, let's look at an example.

<div style="text-align: center"> <img src="/assets/images/blog/2021-07-15/example1.png"/> </div>
<div style="text-align: center"> Example DataBlock Code (taken directly from fastai documentation) </div>
<br>

Like learning any new language, it's confronting at first so let's break it down. Firstly, you'll notice that the data set in question is the [famous MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). With this in mind, take a look at the "blocks" parameter on the first line. You'll notice that it's getting passed an "ImageBlock" that contains the class (cls) PILImageBW which defines explicitly that we're going to pass in an image object where PIL = Python imaging library. Likewise, "CategoryBlock" tells the api that we're also going to pass some sort of label (in this case a number). This is like defining the columns of a dataset (something we're more familiar with) which could be anything meaning that we could have also told the api that we're going to pass in text. The next parameter, "get_items" explicitly states where to get the images from, in this case it's a file path (defined as get_image_files), however, it could also be from a pandas data frame. The final two parameters are more geared towards transformation of the data. The "splitter" param defines how to create a validation data set and "get_y" tells the api, post transformation of the data, what is the "y" or outcome variable called. Note that this is just one example of vision data, however, similar steps apply to pass in text, tabular data, and even audio data.

## Defining a model, training, and prediction
Defining a model doesn't generealise accross all data types exceptionally well since there are many models for different problems. However, defining a model for a specific problem is as easy as oen line of code. On a high level, fastai works off "learner" modelules. For example, for text classification we can use the "text_classifier_learner" to define a Recurrent Neural Network that uses AWD-LSTM (Average SGD Weight-Dropped Long Short-Term Memory). In the example below, observe that at a minimum we can pass in a data loader (dls) and the architecture (AWD_LSTM), however, consulting the documentation you'll discover appreciably more parameters.

<div style="text-align: center"> <img src="/assets/images/blog/2021-07-15/example2.png"/> </div>
<div style="text-align: center"> Example Learner </div>
<br>

Once you have a model, training is as simple as writing (assuming you have a learner called "learn") `learn.fit(n_epoch)`, where "n_epoch" is the number of epochs (amount of cycles through the full training dataset). Thankfully, fastai handles all cross-validation, metrics, and if their's a GPU available (recommend using Kaggle for free GPU usage), fastai automatically uses cuda to pass memory over to the GPU for those sweet efficiency gains. <br>

Finally, as simple as fitting a model, predicting on a new given dataset is as easy as `learn.predict(item)' where item could be an image or text.

# Conclusion
In the overall evolution of machine learning, fastai is not the "be all end all" of neural networks, just like sci-kit learn will not remain the king of machine learning, and (perhaps controversially) Python will not remain the best language for Data Science (check out Julia for instance). However, fastai pushes Data Science accessability bounds ahead, providing the benchmark for future AI/ML packages. The speed and ease to which one can learn fastai is a testiment to the educational prowess of their founders and notably Jeremy Howard and Sylvain Gugger. Speaking of which if you've found this article fascinating please check out [fastai](https://www.fast.ai/) and get started with their legendary course [Deep Learning for Coders](https://course.fast.ai/). Or, if you're more interested in diving straight into things check out their [GitHub repo](https://github.com/fastai/fastbook) which contains their book "Deep Learning for coders" (worth $40 on Amazon) for free as interactive Jupyter notebooks.
